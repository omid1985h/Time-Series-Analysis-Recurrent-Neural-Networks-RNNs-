{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a3a010",
   "metadata": {},
   "source": [
    "# Time-series Guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d5c7d9",
   "metadata": {},
   "source": [
    "1. Time Series Analysis Workflow\n",
    "The typical workflow for time series analysis involves the following steps:\n",
    "Data Collection: Collect time series data.\n",
    "Preprocessing: Prepare the data (handling missing values, normalizing, etc.).\n",
    "Exploratory Data Analysis (EDA): Visualize the data to understand trends, seasonality, and other characteristics.\n",
    "Model Building: Create and train models.\n",
    "Model Evaluation: Assess model performance.\n",
    "Forecasting: Make future predictions.\n",
    "2. Libraries and their Roles\n",
    "pandas: Data manipulation and preprocessing.\n",
    "numpy: Numerical operations for data transformation and preparation.\n",
    "matplotlib: Data visualization (time series plots, residual plots, etc.).\n",
    "sklearn: Machine learning models and tools like scaling, splitting datasets, and regression models.\n",
    "TensorFlow: Deep learning models like Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and other advanced models for time series forecasting.\n",
    "3. Data Collection and Preprocessing\n",
    "Assume you have a dataset with a time series, such as monthly sales data or stock prices. Let’s use pandas for data manipulation and numpy for array operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load time series data (assume 'Date' is a datetime column)\n",
    "df = pd.read_csv('timeseries_data.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# View the first few rows of the data\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Visualize the data\n",
    "df['value'].plot(figsize=(10, 6))\n",
    "plt.title('Time Series Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.show()\n",
    "\n",
    "# Handle missing values (if any) by forward filling\n",
    "df.fillna(method='ffill', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac249513",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Preprocessing:\n",
    "Convert the Date column to a datetime type (if it’s not already).\n",
    "Set the Date as the index.\n",
    "Handle any missing data by forward-filling or interpolation.\n",
    "Normalize/scale if necessary (especially for deep learning models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize the 'value' column to the range [0, 1] using MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df['value'] = scaler.fit_transform(df[['value']])\n",
    "\n",
    "# Split the data into training and testing sets (e.g., 80-20 split)\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_data = df[:train_size]\n",
    "test_data = df[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1952ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Train-Test Split\n",
    "For time series, we cannot randomly split the data like we would with other machine learning problems because of the temporal nature of the data. Ensure that the training set contains data from the beginning to some point in time, and the testing set contains data from a later time point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dcd8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Feature Engineering (Creating Lag Features)\n",
    "A key step in time series analysis is converting the time series data into a supervised learning problem. This involves creating lag features (previous time steps) to predict future values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec571b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_features(data, lag=1):\n",
    "    \"\"\"\n",
    "    Function to create lagged features for time series forecasting.\n",
    "    - `lag`: Number of time steps to use as input features.\n",
    "    \"\"\"\n",
    "    df_lagged = data.copy()\n",
    "    for i in range(1, lag+1):\n",
    "        df_lagged[f'lag_{i}'] = df_lagged['value'].shift(i)\n",
    "    \n",
    "    # Drop rows with missing values created by shifting\n",
    "    df_lagged = df_lagged.dropna()\n",
    "    \n",
    "    return df_lagged\n",
    "\n",
    "# Create lagged features with a lag of 3 (using the previous 3 time steps to predict the next value)\n",
    "train_lagged = create_lagged_features(train_data, lag=3)\n",
    "test_lagged = create_lagged_features(test_data, lag=3)\n",
    "\n",
    "# Separate the features (X) and target (y)\n",
    "X_train, y_train = train_lagged.drop(columns=['value']), train_lagged['value']\n",
    "X_test, y_test = test_lagged.drop(columns=['value']), test_lagged['value']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Model Building: Machine Learning Models\n",
    "Using scikit-learn (sklearn) for Machine Learning Models\n",
    "We can use Random Forest, Linear Regression, or other regression models from sklearn for time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574b3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize the model (Random Forest)\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "print(f'RMSE (Random Forest): {rmse_rf}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8256eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Model Building: Deep Learning with TensorFlow (LSTM)\n",
    "For time series forecasting, Recurrent Neural Networks (RNNs) and LSTMs are often effective. Let's build an LSTM model in TensorFlow for forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18a75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Reshape the data to 3D for LSTM [samples, time steps, features]\n",
    "X_train_3d = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_3d = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Build the LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units=50, activation='relu', input_shape=(X_train_3d.shape[1], X_train_3d.shape[2])))\n",
    "model_lstm.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model_lstm.fit(X_train_3d, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lstm = model_lstm.predict(X_test_3d)\n",
    "\n",
    "# Inverse scale the predictions to original scale\n",
    "y_pred_lstm = scaler.inverse_transform(y_pred_lstm)\n",
    "y_test_actual = scaler.inverse_transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the LSTM model\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test_actual, y_pred_lstm))\n",
    "print(f'RMSE (LSTM): {rmse_lstm}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6368b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "Important Notes for LSTM:\n",
    "Reshaping: LSTMs expect 3D input, where the dimensions are [samples, time steps, features].\n",
    "Training: Adjust the number of epochs and batch size for optimal performance.\n",
    "Activation: You can try other activation functions like tanh or sigmoid depending on the nature of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Model Evaluation and Visualization\n",
    "After training and predicting, you should evaluate the models' performance and visualize the predictions versus the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b98e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test_actual, label='Actual Values')\n",
    "plt.plot(y_pred_lstm, label='LSTM Predictions')\n",
    "plt.title('Time Series Forecasting with LSTM')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Residual plot for model diagnostics\n",
    "residuals = y_test_actual - y_pred_lstm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(residuals)\n",
    "plt.title('Residuals of the LSTM Model')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15abf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Forecasting with the Best Model\n",
    "Once you've identified the best model (based on RMSE or other evaluation metrics), you can use it to forecast future values.\n",
    "For instance, with the LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ca90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the next 12 months (for example)\n",
    "future_steps = 12\n",
    "predictions = []\n",
    "\n",
    "# Start with the last known values\n",
    "input_sequence = test_data.tail(3).values.reshape((1, 1, 3))\n",
    "\n",
    "# Make predictions iteratively for the next time steps\n",
    "for _ in range(future_steps):\n",
    "    next_pred = model_lstm.predict(input_sequence)\n",
    "    predictions.append(next_pred[0][0])\n",
    "    input_sequence = np.roll(input_sequence, -1, axis=2)\n",
    "    input_sequence[0, 0, -1] = next_pred\n",
    "\n",
    "# Inverse transform predictions to original scale\n",
    "predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "\n",
    "# Plot the predictions for the next 12 time steps\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(len(df)), scaler.inverse_transform(df['value'].values.reshape(-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75630b01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
